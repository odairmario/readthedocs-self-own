Build Images
============

This document describes how Read the Docs uses the `Docker Images`_ and how they are named.
Besides, it proposes a new way to create and name them to allow
sharing as many image layers as possible to support more customization while keeping the stability.

.. _Docker Build Images: https://github.com/readthedocs/readthedocs-docker-images


Introduction
------------

We use Docker images to build user's documentation.
Each time a build is triggered, one of our VMs picks the task
and go through different steps:

#. run some application code to spin up a Docker image into a container
#. execute ``git`` inside the container to clone the repository
#. analyze and parse files (``.readthedocs.yaml``) from the repository *outside* the container
#. create the environment and install docs' dependencies inside the container
#. execute build commands inside the container
#. push the output generated by build commands to the storage

*All* those steps depends on specific commands versions: ``git``, ``python``, ``virtualenv``, ``conda``, etc.
Currently, we are pinning only a few of them in our Docker images and that have caused issues
when re-deploying these images with bugfixes: **the images are not reproducible in time**.

.. note::

   The reproducibility of the images will be better once
   https://github.com/readthedocs/readthedocs-docker-images/pull/145 and
   https://github.com/readthedocs/readthedocs-docker-images/pull/146
   get merged but OS packages still won't be 100% the exact same versions.

To allow users to pin the image we ended up exposing three images: ``stable``, ``latest`` and ``testing``.
With that naming, we were able to bugfix issues and add more features
on each image without asking the users to change the image selected in their config file.

Then, when a completely different image appeared and after testing ``testing`` image enough,
we discarded ``stable``, old ``latest`` became the new ``stable`` and old ``testing`` became the new ``latest``.
This produced issues to people pinning their images to any of these names because after this change,
*we changed all the images for all the users* and many build issues arrised!


Goals
-----

* release completely new Docker images without forcing users to change their pinned image
* allow users to stick with an image "forever" (~years)
* use a ``base`` image with the dependencies that don't change frequently (OS and base requirements)
* ``base`` image naming is tied to the OS version (e.g. Ubuntu LTS)
* allow us to update a Python version without affecting the ``base`` image
* reduce size on builder VM disks by sharing Docker image layers
* allow users to specify extra dependencies (apt packages, node, rust, etc)
* allow use "limited" custom images for users by sharing most layers
* automatically build & push *all* images on commit
* deprecate ``stable``, ``latest`` and ``testing``


New build image structure
-------------------------

.. Taken from https://github.com/readthedocs/readthedocs-docker-images/blob/master/Dockerfile

* ``ubuntu20-base``
  * labels
  * environment variables
  * system dependencies
  * install requirements
  * LaTeX dependencies (for PDF generation)
  * other languages version managers (``pyenv``, ``nodenv``, etc)
  * UID and GID

The following images all are based on ``ubuntu20-base``:

* ``ubuntu20-py27``
* ``ubuntu20-py36``
* ``ubuntu20-py37``
* ``ubuntu20-py38``
* ``ubuntu20-py39``
* ``ubuntu20-conda47`` (contains ``mamba`` executable as well)

Note that all these images only need to run ``pyenv install ${PYTHON_VERSION}``
to install a specific Python/Conda version.

.. Build all these images with Docker

   docker build -t readthedocs/build:ubuntu20-base -f Dockerfile.base .
   docker build -t readthedocs/build:ubuntu20-py39 -f Dockerfile.py39 .
   docker build -t readthedocs/build:ubuntu20-conda47 -f Dockerfile.conda47 .

   Check the shared space between images
   docker system df --verbose | grep -E 'SHARED SIZE|readthedocs'


Specifying extra users' dependencies
------------------------------------

Different users may have different requirements. We were already requested to install
``swig``, ``imagemagick``, ``libmysqlclient-dev``, ``lmod``, ``rust``, ``poppler-utils``, etc.

People with specific dependencies will be able to install them as APT packages or as extras
using ``.readthedocs.yaml`` config file. Example:

.. code:: yaml

   build:
   image: ubuntu20-py39
     apt:
       - swig
       - imagemagick
     extras:
       - node==14.16
       - rust==1.46.0


.. note:: Idea for implementation

   Once this config file is parsed, the builder builds a Docker image on-demand with a command similar to:

   .. console::

      docker build \
        --tag ${BUILD_ID} \
        --file Dockerfile.custom \
        --build-arg RTD_IMAGE=ubuntu20-py39
        --build-arg RTD_NODE_VERSION=14.16 \
        --build-arg RTD_RUST_VERSION=1.46.0 \
        --build-arg RTD_APT_PACKAGES="swig imagemagick"

   using ``Dockerfile.custom`` that has the following content:

   .. code:: Dockerfile

      ARG RTD_IMAGE
      FROM readthedocs:${RTD_IMAGE}

      ARG RTD_NODE_VERSION
      ARG RTD_RUST_VERSION
      ARG RTD_APT_PACKAGES

      USER root
      WORKDIR /

      # Install extras
      RUN apt-get update
      RUN apt-get install -y ${RTD_APT_PACKAGES}

      USER docs
      WORKDIR /home/docs

      # Install ``nodejs``
      RUN nodenv install ${RTD_NODE_VERSION}
      RUN nodenv global ${RTD_NODE_VERSION}

      # Install ``rust``
      RUN curl https://sh.rustup.rs -sSf | sh -s -- -y --default-toolchain ${RTD_RUST_VERSION}
      ENV PATH="/home/docs/.cargo/bin:$PATH"

   Building this image should be pretty fast since all the requirements to install these extra packages
   are already installed and all of them are pre-compiles binaries. It will take the time it takes to download them.


Updating versions over time
---------------------------

How do we add/upgrade a Python version?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Python patch versions can be upgraded on the affected image.
As the ``base`` image won't change for this case, it will only modify the layers after it.
All the OS package versions will remain the same.

In case we need to *add* a new Python version, we just need to build a new image based on ``base``:
``ubuntu20-py310`` that will contain Python 3.10 and none of the other images are affected.


How do we upgrade system versions?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We usually don't upgrade these dependencies unless we upgrade the Ubuntu version.
So, they will be only upgraded when we go from Ubuntu 18.04 LTS to Ubuntu 20.04 LTS for example.

Examples of these versions are:

* doxygen
* git
* subversion
* pandoc
* swig
* latex

This case will introduce a new ``base`` image. Example, ``ubuntu22-base`` in 2022.
Note that these images will be completely isolated from the rest and don't require them to rebuild.
This also allow us to test new Ubuntu versions without breaking people's builds.

How do we add an extra requirement?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In case we need to add an extra requirement to the ``base`` image,
we will need to rebuild all of them.
The new image may have different package versions since there may be updates on the Ubuntu repositories.
This conveys some small risk here, but in general we shouldn't require to add packages to the base images.

Users with specific requirements could use ``build.apt`` and/or ``build.extras`` in the config file.

How do we remove an old Python version?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At some point an old version of Python will be deprecated (eg. 3.4) and will be removed.
To achieve this, we can just remove the Docker image affected: ``ubuntu20-py34``,
once there are no users depending on it anymore.


Deprecation plan
----------------

It seems we have ~50Gb free on builders disks.
Considering that the new images will be sized approximately (built locally as test):

* ``ubuntu20-base``: ~
* ``ubuntu20-py27``: ~
* ``ubuntu20-py39``: ~
* ``ubuntu20-conda47``: ~

which is about ~10Gb in total, we will still have space to support multiple custom images.

We could keep ``stable``, ``latest`` and ``testing`` for some time without worry too much.
New projects shouldn't be able to select these images and they will be forced to use ``ubuntu20``
or any other custom image.

We may want to keep the two latest Ubuntu LTS releases available in production.
At the moment of writing this they are:

* Ubuntu 18.04 LTS (our ``stable``, ``latest`` and ``testing`` images)
* Ubuntu 20.04 LTS (our new ``ubuntu20``)

Once Ubuntu 22.04 LTS is released, we should deprecate Ubuntu 18.04 LTS,
and give users 6 months to migrate to a newer image.


Conclusion
----------

I don't think we need to differentiate the images by its state (stable, latest, testing)
but by its main base differences: OS and Python version.
The version of the OS will change many library versions,
LaTeX dependencies, basic required commands like git and more,
that doesn't seem to be useful to have the same OS version with different states.

"Limited" custom Docker images is something that will cover most of the support requests we have had in the past
and allow users to use our platform in a controlled way for us.
Exposing users how we want them to use our platform will allow us to be able to maintain it longer,
than given them totally freedom on the Docker image.

"Non limited" custom Docker images is out of the scope of this document,
but could be done in a similar way as the "limited" on-demand Docker images.
However, there are other aspects like persistence of the image between builds
that needs to be considered as well.
